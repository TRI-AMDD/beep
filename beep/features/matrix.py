
import copy
import hashlib
from typing import List, Union

import pandas as pd
from monty.json import MSONable, MontyDecoder
from monty.serialization import loadfn, dumpfn

from beep.features.featurizer import BEEPFeaturizer, BEEPPerCycleFeaturizer, BEEPEarlyCyclesFeaturizer


class BEEPFeatureMatrixError(BaseException):
    """ Raise when there is a BEEP-specific problem with a dataset"""
    pass


class BEEPFeatureMatrix(MSONable):
    """
    Create an array composed of BEEPFeaturizer objects.

    The array may either be:

    PER-CYCLER-RUN, using BEEPEarlyCyclesFeaturizer.
    One feature vector per cycler file, resulting in an array w. dimenions:
        (n battery cycler files) x (k features)

    OR:

    PER-CYCLE, using BEEPPerCycleFeaturizer.
    One feature vector per cycle, resulting in an array w. dimensions:
        (n total cycles) x (k features)

    Sets of featurizer objects must exclusively belong to EITHER of these
    two paradigms (base classes), but may not be mixed.

    So a set of featurizrs may be -per-cycler-file OR per-cycle, but not
    both.

    Args:
        beepfeaturizers ([BEEPFeaturizer]): A list of BEEPFeaturizer objects,
            either ALL BEEPEarlyCyclesFeaturizer child objects OR ALL
            BEEPPerCycleFeaturizer child objects.

    """

    OP_DELIMITER = "::"

    def __init__(
            self,
            beepfeaturizers: List[Union[BEEPPerCycleFeaturizer, BEEPEarlyCyclesFeaturizer]]
    ):

        if beepfeaturizers:
            bfs_types_per_cycle = [bf.PER_CYCLE for bf in beepfeaturizers]

            # the array should be either all True or all False
            if all(bfs_types_per_cycle):
                self.per_cycle = True
            elif not any(bfs_types_per_cycle):
                self.per_cycle = False
            else:
                raise TypeError(
                    f"Featurizer types are mixed!\n"
                    f"BEEPFeatureMatrix can only use EITHER a set of ALL "
                    f"BEEPAllCyclesFeaturizers OR a set of ALL "
                    f"BEEPPerCycleFeaturizers.")
            dfs_by_file = {bf.paths.get("structured", "no file found"): [] for bf in beepfeaturizers}

            unique_features = {}
            for i, bf in enumerate(beepfeaturizers):

                if bf.features is None:
                    raise BEEPFeatureMatrixError(
                        f"BEEPFeaturizer {bf} has not created features")
                else:
                    bfcn = bf.__class__.__name__

                    fname = bf.paths.get("structured", None)
                    if not fname:
                        raise BEEPFeatureMatrixError(
                            "Cannot join features automatically as no linking can be done "
                            "based on original structured filename."
                        )


                    # Ensure no per-cycle featurizer is missing required special columns for merge
                    if self.per_cycle:
                        missing_special_columns = set()
                        for sf in BEEPPerCycleFeaturizer.SPECIAL_COLUMNS:
                            if sf not in bf.features.columns:
                                missing_special_columns.add(sf)
                        if missing_special_columns:
                            raise BEEPFeatureMatrixError(
                                f"Per cycle featurizer object missing special columns: {missing_special_columns}"
                            )

                    # Check for any possible feature collisions using identical featurizers
                    # on identical files

                    # sort params for this featurizer obj by key
                    params = sorted(list(bf.hyperparameters.items()), key=lambda x: x[0])

                    # Prevent identical features from identical input files
                    # create a unique operation string for the application of this featurizer
                    # on a specific file, this op string will be the same as long as
                    # the featurizer class name, hyperparameters, and class are the same

                    param_str = "-".join([f"{k}:{v}" for k, v in params])
                    param_hash = hashlib.sha256(
                        param_str.encode("utf-8")).hexdigest()

                    # Get an id for this featurizer operation (including hyperparameters)
                    # regardless of the file it is applied on
                    feature_op_id = f"{bfcn}{self.OP_DELIMITER}{param_hash}"

                    # Get an id for this featurizer operation (including hyperparameters)
                    # on THIS SPECIFIC file.
                    file_feature_op_id = f"{fname}{self.OP_DELIMITER}{bfcn}{self.OP_DELIMITER}{param_hash}"

                    # Get a unique id for every feature generated by a specific
                    # featurizer on a specific file.
                    this_file_feature_columns_ids = \
                        [
                            f"{file_feature_op_id}{self.OP_DELIMITER}{c}" for c
                            in bf.features.columns
                        ]

                    # Check to make sure there are no duplicates of the exact same feature for
                    # the exact same featurizer with the exact same hyperparameters on the exact
                    # same file.
                    collisions = {c: f for c, f in unique_features.items() if
                                  c in this_file_feature_columns_ids}
                    if collisions:
                        raise BEEPFeatureMatrixError(
                            f"Multiple features generated with identical classes and identical hyperparameters"
                            f" attempted to be joined into same dataset; \n"
                            f"{bfcn} features collide with existing: \n{collisions}"
                        )
                    for c in this_file_feature_columns_ids:
                        unique_features[c] = bfcn

                    # Create consistent scheme for naming features regardless of file
                    # Only rename non-special column names
                    df = copy.deepcopy(bf.features)
                    special_column_names = BEEPPerCycleFeaturizer.SPECIAL_COLUMNS if self.per_cycle else set()
                    consistent_column_names = []

                    for c in df.columns:
                        if c in special_column_names:
                            consistent_column_names.append(c)
                        else:
                            consistent_column_names.append(f"{c}{self.OP_DELIMITER}{feature_op_id}")
                    df.columns = consistent_column_names

                    # ensure cycle_index and diag_pos are integers
                    if self.per_cycle:
                        for col in BEEPPerCycleFeaturizer.SPECIAL_COLUMNS:
                            df[col] = df[col].astype(int)
                        df["filename"] = [fname] * df.shape[0]

                    else:
                        df.index = [fname] * df.shape[0]
                        df.index.rename("filename", inplace=True)

                    dfs_by_file[fname].append(df)

            blocks = []
            self.dfs_by_file = dfs_by_file
            indexing_cols = ["filename"] + list(BEEPPerCycleFeaturizer.SPECIAL_COLUMNS)

            # concat dfs by file across columns
            for filename, dfs in dfs_by_file.items():
                if self.per_cycle:
                    rows = dfs[0]
                    for df in dfs[1:]:
                        rows = pd.merge(
                            rows,
                            df,
                            how="outer",
                            on=indexing_cols
                        )
                    rows = rows.reset_index().sort_values(["cycle_index"])
                    feature_cols = sorted([f for f in rows.columns if f not in indexing_cols])
                    rows = rows[indexing_cols + feature_cols]

                else:
                    rows = pd.concat(dfs, axis=1)
                    rows = rows[sorted(rows.columns)]

                blocks.append(rows)

            # concat all dfs for all files across rows
            self.matrix = pd.concat(blocks, axis=0)

        else:
            self.matrix = None

        self.featurizers = beepfeaturizers

    def as_dict(self):
        """Serialize a BEEPFeatureMatrix as a dictionary.

        Must not be loaded from legacy.

        Returns:
            (dict): corresponding to dictionary for serialization.

        """

        return {
            "@module": self.__class__.__module__,
            "@class": self.__class__.__name__,

            # Core parts of BEEPFeaturizer
            "featurizers": [f.as_dict() for f in self.featurizers],
            "matrix": self.matrix.to_dict("list"),
        }

    @classmethod
    def from_dict(cls, d):
        """Create a BEEPFeatureMatrix object from a dictionary.

        Args:
            d (dict): dictionary represenation.

        Returns:
            beep.structure.ProcessedCyclerRun: deserialized ProcessedCyclerRun.
        """
        # no need for original datapaths, as their ref paths should
        # be in the subobjects
        featurizers = [MontyDecoder().process_decoded(f) for f in
                       d["featurizers"]]
        return cls(featurizers)

    @classmethod
    def from_json_file(cls, filename):
        """Load a structured run previously saved to file.

        .json.gz files are supported.

        Loads a BEEPFeatureMatrix from json.

        Can be used in combination with files serialized with BEEPFeatures.to_json_file.

        Args:
            filename (str, Pathlike): a json file from a structured run, serialzed with to_json_file.

        Returns:
            None
        """
        return loadfn(filename)

    def to_json_file(self, filename):
        """Save a BEEPFeatureMatrix to disk as a json.

        .json.gz files are supported.

        Not named from_json to avoid conflict with MSONable.from_json(*)

        Args:
            filename (str, Pathlike): The filename to save the file to.
            omit_raw (bool): If True, saves only structured (NOT RAW) data.
                More efficient for saving/writing to disk.

        Returns:
            None
        """
        d = self.as_dict()
        dumpfn(d, filename)


# class BEEPCycleFeatureMatrix(MSONable):
#     """
#     Create an ((n battery cycler files) x (j cycles)) x (k features)  array composed of
#     m BEEPFeaturizer objects.
#
#     Args:
#         beepfeaturizers ([BEEPFeaturizer]): A list of BEEPFeaturizer objects
#
#     """
#
#     OP_DELIMITER = "::"
#
#     def __init__(self, beepfeaturizers: List[BEEPFeaturizer]):
#
#         if beepfeaturizers:
#             # initialize emtpy dict of file names
#             dfs_by_file = {os.path.basename(
#                 bf.paths.get("structured", "no file found")
#             )[0:-19]: pd.DataFrame(
#                 columns=['filename', 'cycle_index', 'diag_pos']
#                 ) for bf in beepfeaturizers}
#             # big_df_rows = {bf.__class__.__name__: [] for bf in beepfeaturizers}
#             unique_features = {}
#             for i, bf in enumerate(beepfeaturizers):
#                 if bf.features is None:
#                     raise BEEPFeatureMatrixError(
#                         f"BEEPFeaturizer {bf} has not created features")
#
#                 #                 elif bf.features.shape[0] != 1:
#                 #                     raise BEEPFeatureMatrixError(f"BEEPFeaturizer {bf} features are not 1-dimensional.")
#                 else:
#                     bfcn = bf.__class__.__name__
#
#                     #                     fname = bf.paths.get("structured", None)
#                     fname = os.path.basename(bf.paths['structured'])[0:-19]
#                     if not fname:
#                         raise BEEPFeatureMatrixError(
#                             "Cannot join features automatically as no linking can be done "
#                             "based on original structured filename."
#                         )
#
#                     # Check for any possible feature collisions using identical featurizers
#                     # on identical files
#
#                     # sort params for this featurizer obj by key
#                     params = sorted(list(bf.hyperparameters.items()),
#                                     key=lambda x: x[0])
#
#                     # Prevent identical features from identical input files
#                     # create a unique operation string for the application of this featurizer
#                     # on a specific file, this op string will be the same as long as
#                     # the featurizer class name, hyperparameters, and class are the same
#
#                     param_str = "-".join([f"{k}:{v}" for k, v in params])
#                     param_hash = hashlib.sha256(
#                         param_str.encode("utf-8")).hexdigest()
#
#                     # Get an id for this featurizer operation (including hyperparameters)
#                     # regardless of the file it is applied on
#                     feature_op_id = f"{bfcn}{self.OP_DELIMITER}{param_hash}"
#
#                     # Get an id for this featurizer operation (including hyperparameters)
#                     # on THIS SPECIFIC file.
#                     file_feature_op_id = f"{fname}{self.OP_DELIMITER}{bfcn}{self.OP_DELIMITER}{param_hash}"
#
#                     # Get a unique id for every feature generated by a specific
#                     # featurizer on a specific file.
#                     this_file_feature_columns_ids = \
#                         [
#                             f"{file_feature_op_id}{self.OP_DELIMITER}{c}" for c
#                             in bf.features.columns
#                         ]
#
#                     # Check to make sure there are no duplicates of the exact same feature for
#                     # the exact same featurizer with the exact same hyperparameters on the exact
#                     # same file.
#                     collisions = {c: f for c, f in unique_features.items() if
#                                   c in this_file_feature_columns_ids}
#                     if collisions:
#                         raise BEEPFeatureMatrixError(
#                             f"Multiple features generated with identical classes and identical hyperparameters"
#                             f" attempted to be joined into same dataset; \n"
#                             f"{bfcn} features collide with existing: \n{collisions}"
#                         )
#                     for c in this_file_feature_columns_ids:
#                         unique_features[c] = bfcn
#
#                     # Create consistent scheme for naming features regardless of file
#                     df = copy.deepcopy(bf.features)
#                     consistent_column_names = [
#                         f"{c}{self.OP_DELIMITER}{feature_op_id}" for c in
#                         df.columns]
#                     df.columns = consistent_column_names
#
#                     #                     df.index = [fname] * df.shape[0]
#                     #                     df.index.rename("filename", inplace=True)
#
#                     # create filename column to merge on
#                     df['filename'] = os.path.basename(bf.paths['structured'])[
#                                      0:-19]
#
#                     #                     df = df.reset_index(drop=True)
#
#                     # remove hash from cycle_index and diag_pos column
#                     cycle_index_col = [col for col in df.columns if
#                                        'cycle_index' in col]
#                     df.rename(columns={cycle_index_col[0]: 'cycle_index'},
#                               inplace=True)
#
#                     # remove hash from diag_pos column
#                     diag_pos_col = [col for col in df.columns if
#                                     'diag_pos' in col]
#                     df.rename(columns={diag_pos_col[0]: 'diag_pos'},
#                               inplace=True)
#
#                     # ensure cycle_index and diag_pos are integers
#                     df['cycle_index'] = df['cycle_index'].astype(int)
#                     df['diag_pos'] = df['diag_pos'].astype(int)
#
#                     # append each BEEPFeaturizer df to the corresponding cell dict entry
#                     #                     dfs_by_file[fname].append(df)
#                     dfs_by_file[fname] = dfs_by_file[fname].merge(
#                         df, how='outer',
#                         on=['filename', 'cycle_index', 'diag_pos']).sort_values(
#                         'cycle_index').reset_index(drop=True)
#             #                     dfs_by_file[fname] = pd.concat(
#             #                         [dfs_by_file[fname],df],
#             #                         axis=1,join='outer',ignore_index=True,
#             #                         keys=['filename'])
#             #                     self.dfs_by_file = dfs_by_file
#             #                     self.df = df
#             #             return None
#
#             rows = []
#             self.matrix = pd.DataFrame()
#             for filename, dfs in dfs_by_file.items():
#                 #                 row = pd.concat([row,dfs], axis=1)
#                 #                 row = row[sorted(row.columns)]
#                 #                 rows.append(row)
#                 self.matrix = pd.concat([self.matrix, dfs], axis=0,
#                                         ignore_index=True,
#                                         join='outer')  # , keys=['filename']
#
#         else:
#             self.matrix = None
#
#         self.featurizers = beepfeaturizers
#
#     def as_dict(self):
#         """Serialize a BEEPDatapath as a dictionary.
#
#         Must not be loaded from legacy.
#
#         Returns:
#             (dict): corresponding to dictionary for serialization.
#
#         """
#
#         return {
#             "@module": self.__class__.__module__,
#             "@class": self.__class__.__name__,
#
#             # Core parts of BEEPFeaturizer
#             "featurizers": [f.as_dict() for f in self.featurizers],
#             "matrix": self.matrix.to_dict("list"),
#         }
#
#     @classmethod
#     def from_dict(cls, d):
#         """Create a BEEPDatapath object from a dictionary.
#
#         Args:
#             d (dict): dictionary represenation.
#
#         Returns:
#             beep.structure.ProcessedCyclerRun: deserialized ProcessedCyclerRun.
#         """
#         # no need for original datapaths, as their ref paths should
#         # be in the subobjects
#         featurizers = [MontyDecoder().process_decoded(f) for f in
#                        d["featurizers"]]
#         return cls(featurizers)
#
#     @classmethod
#     def from_json_file(cls, filename):
#         """Load a structured run previously saved to file.
#
#         .json.gz files are supported.
#
#         Loads a BEEPFeatureMatrix from json.
#
#         Can be used in combination with files serialized with BEEPFeatures.to_json_file.
#
#         Args:
#             filename (str, Pathlike): a json file from a structured run, serialzed with to_json_file.
#
#         Returns:
#             None
#         """
#         return loadfn(filename)
#
#     def to_json_file(self, filename):
#         """Save a BEEPFeatureMatrix to disk as a json.
#
#         .json.gz files are supported.
#
#         Not named from_json to avoid conflict with MSONable.from_json(*)
#
#         Args:
#             filename (str, Pathlike): The filename to save the file to.
#             omit_raw (bool): If True, saves only structured (NOT RAW) data.
#                 More efficient for saving/writing to disk.
#
#         Returns:
#             None
#         """
#         d = self.as_dict()
#         dumpfn(d, filename)